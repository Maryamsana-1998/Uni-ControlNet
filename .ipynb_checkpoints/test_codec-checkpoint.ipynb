{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d0db233-1963-483a-bb53-79fd0b847075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from pytorch_lightning import seed_everything\n",
    "import glob\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Importing specific utilities\n",
    "from src.test.test_codec import process\n",
    "from utils.share import *\n",
    "import utils.config as config\n",
    "from annotator.util import resize_image, HWC3\n",
    "from annotator.canny import CannyDetector\n",
    "from models.util import create_model, load_state_dict\n",
    "from models.ddim_hacked import DDIMSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d436e3f7-5cbb-41a9-857b-a5a1898079ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UniControlNet: Running in eps-prediction mode\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Loaded model config from [configs/uni_v15.yaml]\n",
      "Loaded state_dict from [/data/maryam.sana/vimeo_unicontrol/Uni-ControlNet/checkpoints/vimeo_8/uni_v8.ckpt]\n"
     ]
    }
   ],
   "source": [
    "ckpt_path= \"/data/maryam.sana/vimeo_unicontrol/Uni-ControlNet/checkpoints/vimeo_8/uni_v8.ckpt\"\n",
    "config_path =\"configs/uni_v15.yaml\"\n",
    "prompt = \"A beautiful blonde girl with pink lipstick\"\n",
    "model = create_model(config_path).cpu()\n",
    "model.load_state_dict(load_state_dict(ckpt_path, location=\"cuda\"))\n",
    "model = model.cuda()\n",
    "\n",
    "a_prompt = \"best quality, extremely detailed\"\n",
    "n_prompt = \"longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality\"\n",
    "num_samples = 1\n",
    "image_resolution = 512\n",
    "ddim_steps = 50\n",
    "strength = 1\n",
    "scale = 7.5\n",
    "seed = 42\n",
    "eta = 0.0\n",
    "global_strength = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c769dd8f-a23b-4a74-9003-7b0d26e47af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = sorted(glob.glob('../vimeo_unicontrol/Uni-ControlNet/data/UVG/Beauty/*.png'))[1:20]\n",
    "optical_flows = sorted(glob.glob('../vimeo_unicontrol/Uni-ControlNet/data/UVG/optical_flow/Beauty_reconstructed/*.png'))\n",
    "encoded_frame = cv2.imread(\"/data/maryam.sana/vimeo_unicontrol/Uni-ControlNet/data/UVG/Beauty/quality_4/im00001.png\")  # Load encoded frame\n",
    "# print(image_paths,optical_flows)\n",
    "prompt = \"A beautiful blonde girl with pink lipstick\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb118adc-01c9-4a4f-bcf1-43abadff1376",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@235.734] global loadsave.cpp:241 findDecoder imread_('path/to/encoded_frame.png'): can't open/read file: check file path/integrity\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512, 3)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m     frame_image \u001b[38;5;241m=\u001b[39m predictions[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Use the last prediction for subsequent frames\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Use the `process` function\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcanny_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_resolution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mddim_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrength\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43meta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mglobal_strength\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m pred_img \u001b[38;5;241m=\u001b[39m pred[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Ensure the images are of the same size\u001b[39;00m\n",
      "File \u001b[0;32m/data/maryam.sana/Uni-ControlNet/src/test/test_codec.py:62\u001b[0m, in \u001b[0;36mprocess\u001b[0;34m(model, canny_image, frame_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, ddim_steps, strength, scale, seed, eta, global_strength)\u001b[0m\n\u001b[1;32m     59\u001b[0m canny_detected_map \u001b[38;5;241m=\u001b[39m HWC3(canny_image)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(canny_detected_map\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 62\u001b[0m frame_map \u001b[38;5;241m=\u001b[39m  cv2\u001b[38;5;241m.\u001b[39mresize(\u001b[43mHWC3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_image\u001b[49m\u001b[43m)\u001b[49m, (W, H))\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(frame_map\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     65\u001b[0m content_emb \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m768\u001b[39m))\n",
      "File \u001b[0;32m/data/maryam.sana/Uni-ControlNet/annotator/util.py:10\u001b[0m, in \u001b[0;36mHWC3\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mHWC3\u001b[39m(x):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39muint8\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m     12\u001b[0m         x \u001b[38;5;241m=\u001b[39m x[:, :, \u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "encoded_frame = cv2.cvtColor(encoded_frame,cv2.COLOR_BGR2RGB)\n",
    "pred_folder = \"test_codec/beauty/\"\n",
    "os.makedirs(pred_folder, exist_ok=True)\n",
    "\n",
    "# Load images\n",
    "original_images = [cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB) for path in image_paths]\n",
    "optical_flows_images = [cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB) for path in optical_flows]\n",
    "\n",
    "# Initialize previous frame as the encoded frame for the second image\n",
    "previous_frame = encoded_frame\n",
    "\n",
    "# List to store predictions\n",
    "predictions = []\n",
    "\n",
    "# Process images\n",
    "for i in range(len(image_paths)):\n",
    "    original_image = original_images[i]\n",
    "    canny_image = optical_flows_images[i]\n",
    "\n",
    "    # Use optical flow for the second image and previous prediction for subsequent frames\n",
    "    if i == 0:\n",
    "        frame_image = previous_frame  # Use the encoded frame for the first image\n",
    "    else:\n",
    "        frame_image = predictions[-1]  # Use the last prediction for subsequent frames\n",
    "\n",
    "    # Use the `process` function\n",
    "    pred = process(\n",
    "        model,\n",
    "        canny_image,\n",
    "        frame_image,\n",
    "        prompt,\n",
    "        a_prompt,\n",
    "        n_prompt,\n",
    "        num_samples,\n",
    "        image_resolution,\n",
    "        ddim_steps,\n",
    "        strength,\n",
    "        scale,\n",
    "        seed,\n",
    "        eta,\n",
    "        global_strength,\n",
    "    )\n",
    "    pred_img = pred[0][0]\n",
    "\n",
    "    # Ensure the images are of the same size\n",
    "    if original_image.shape != pred_img.shape:\n",
    "        pred_img = cv2.resize(pred_img, (original_image.shape[1], original_image.shape[0]))\n",
    "\n",
    "    # Save the prediction for the next frame\n",
    "    predictions.append(pred_img)\n",
    "\n",
    "    # Save prediction image to disk\n",
    "    pred_image_path = os.path.join(pred_folder, f\"im{i + 1}_pred.png\")\n",
    "    cv2.imwrite(pred_image_path, cv2.cvtColor(pred_img, cv2.COLOR_RGB2BGR))\n",
    "    print(f\"Saved prediction image: {pred_image_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00bcb4d-d1ba-46df-b750-de1aac601b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
