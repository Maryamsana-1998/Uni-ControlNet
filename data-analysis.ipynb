{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ac7b084-fc1e-41d7-a4e9-bd36a0d43863",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data analysis of spring dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05697a48-508b-466e-996a-092098e0428b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to read and extract optical flow data from an HDF5 (.flo5) file\n",
    "def read_flo5_file(file_path):\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        # Assuming the dataset key inside the HDF5 file contains flow data\n",
    "        flow_data = f['flow'][:]  # Change 'flow' to your actual dataset key\n",
    "        \n",
    "        # Flow data is expected to be [H, W, 2] where the last dimension contains u and v components\n",
    "        return flow_data\n",
    "\n",
    "# Function to convert optical flow to RGB using HSV color space\n",
    "def flow_to_color(flow):\n",
    "    \"\"\"Convert optical flow to RGB image using HSV color model.\"\"\"\n",
    "    h, w = flow.shape[:2]\n",
    "    hsv = np.zeros((h, w, 3), dtype=np.float32)\n",
    "    \n",
    "    # Get flow components (u and v)\n",
    "    flow_x = flow[:, :, 0]\n",
    "    flow_y = flow[:, :, 1]\n",
    "\n",
    "    # Ensure the data types are float32 (CV_32F)\n",
    "    flow_x = flow_x.astype(np.float32)\n",
    "    flow_y = flow_y.astype(np.float32)\n",
    "\n",
    "    # Print flow data types and shapes for debugging\n",
    "    print(f\"flow_x dtype: {flow_x.dtype}, shape: {flow_x.shape}\")\n",
    "    print(f\"flow_y dtype: {flow_y.dtype}, shape: {flow_y.shape}\")\n",
    "\n",
    "    # Convert flow vectors to magnitude and angle\n",
    "    magnitude, angle = cv2.cartToPolar(flow_x, flow_y)\n",
    "\n",
    "    # Normalize magnitude to [0, 1]\n",
    "    hsv[..., 0] = angle * 180 / np.pi / 2  # Hue represents the angle\n",
    "    hsv[..., 1] = 1  # Full saturation\n",
    "    hsv[..., 2] = cv2.normalize(magnitude, None, 0, 1, cv2.NORM_MINMAX)  # Value represents the magnitude\n",
    "\n",
    "    # Convert HSV to RGB for visualization\n",
    "    rgb_flow = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n",
    "    \n",
    "    return rgb_flow\n",
    "\n",
    "# Function to process .flo5 file and visualize/save the optical flow as an image\n",
    "def process_flo5_file(flo5_file, output_image):\n",
    "    # Read the optical flow data from the .flo5 (HDF5) file\n",
    "    flow = read_flo5_file(flo5_file)\n",
    "    \n",
    "    # Convert the optical flow to a color representation\n",
    "    flow_rgb = flow_to_color(flow)\n",
    "    \n",
    "    # Save the flow visualization as an image\n",
    "    cv2.imwrite(output_image, (flow_rgb * 255).astype(np.uint8))\n",
    "    print(f\"Flow visualization saved as {output_image}\")\n",
    "    \n",
    "    # Display the flow visualization\n",
    "    plt.imshow(flow_rgb)\n",
    "    plt.title('Optical Flow Visualization')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to your .flo5 (HDF5) file\n",
    "    flo5_file = '/data/maryam.sana/datazips/spring/train/0008/flow_FW_left/flow_FW_left_0007.flo5'  # Replace with the actual path\n",
    "    \n",
    "    # Output image path\n",
    "    output_image = 'flow_visualization.png'\n",
    "    \n",
    "    # Process and visualize the .flo5 file\n",
    "    # process_flo5_file(flo5_file, output_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1ceb887-65a1-4e31-b681-da8d2ce04b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from src.train.video_dataset import *\n",
    "import torch\n",
    "\n",
    "# Define the dataset root directory and other parameters\n",
    "root_dir = \"/data/maryam.sana/datazips/spring/\"\n",
    "frame_dir = \"frame_left\"\n",
    "optical_flow_dir = \"flow_images\"\n",
    "encoded_frame_dir = \"encoded_left\"\n",
    "resolution = 512  # Example resolution\n",
    "global_type_list = []  # Populate with your global types as required\n",
    "drop_txt_prob = 0.1  # Example probabilities\n",
    "keep_all_cond_prob = 0.8\n",
    "drop_all_cond_prob = 0.1\n",
    "drop_each_cond_prob = [0.5,0.5]\n",
    "\n",
    "# Initialize the custom dataset\n",
    "dataset = UniVideoDataset(\n",
    "    root=root_dir,\n",
    "    frame_dir=frame_dir,\n",
    "    optical_flow_dir=optical_flow_dir,\n",
    "    encoded_frame_dir=encoded_frame_dir,\n",
    "    global_type_list=global_type_list,\n",
    "    resolution=resolution,\n",
    "    drop_txt_prob=drop_txt_prob,\n",
    "    keep_all_cond_prob=keep_all_cond_prob,\n",
    "    drop_all_cond_prob=drop_all_cond_prob,\n",
    "    drop_each_cond_prob=drop_each_cond_prob,\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # Separate out the parts of the batch that are strings and numpy arrays/tensors\n",
    "    # print(batch.shape)\n",
    "    txt_batch = [item['txt'] for item in batch]  # Extract 'txt'\n",
    "\n",
    "    # Extract the image data (jpg) and ensure it has shape [5, 512, 512, 3] without adding a batch dimension\n",
    "    jpg_batch = torch.cat([torch.tensor(item['jpg']) for item in batch], dim=0) if len(batch) > 1 else torch.tensor(batch[0]['jpg'])\n",
    "\n",
    "    # Handle local and global conditions similarly\n",
    "    local_conditions_batch = torch.cat([torch.tensor(item['local_conditions']) for item in batch], dim=0) if len(batch) > 1 else torch.tensor(batch[0]['local_conditions'])\n",
    "    global_conditions_batch = torch.cat([torch.tensor(item['global_conditions']) for item in batch], dim=0) if len(batch) > 1 else torch.tensor(batch[0]['global_conditions'])\n",
    "\n",
    "    # Return the combined dictionary\n",
    "    return {\n",
    "        'jpg': jpg_batch,  # This will have shape [5, 512, 512, 3] for each item\n",
    "        'txt': txt_batch,  # Keep as a list of strings\n",
    "        'local_conditions': local_conditions_batch,\n",
    "        'global_conditions': global_conditions_batch\n",
    "    }\n",
    "    \n",
    "# Initialize the DataLoader\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    collate_fn=custom_collate_fn,\n",
    "    num_workers=4,\n",
    "    batch_size=1,\n",
    "    pin_memory=True,\n",
    "    shuffle=True,\n",
    "    persistent_workers=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9933fa9b-90e6-4608-9139-12e073443bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([3, 512, 512, 3])\n",
      "Local conditions shape: torch.Size([3, 512, 512, 6])\n",
      "Text data: [array(['predict next image', 'predict next image', 'predict next image'],\n",
      "      dtype='<U18')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for batch in dataloader:\n",
    "    # batch is now a dictionary, not a list of dictionaries\n",
    "    images = batch['jpg']  # Image data, should be a tensor\n",
    "    txt = batch['txt']  # Text/annotation data, a list of strings\n",
    "    local_conditions = batch['local_conditions']  # Local conditions (optical flow and encoded frame)\n",
    "    global_conditions = batch['global_conditions']  # Global conditions\n",
    "\n",
    "    # Now you can iterate through the data if necessary or directly use it\n",
    "    print(\"Images shape:\", images.shape)  # Should be a tensor\n",
    "    print(\"Local conditions shape:\", local_conditions.shape)  # Should be a tensor\n",
    "    # print(\"Global conditions shape:\", global_conditions.shape)  # Should be a tensor\n",
    "    print(\"Text data:\", txt)  # List of text annotations or empty strings\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ab97a7-dc2d-4db0-9f81-cf34cb026d65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
