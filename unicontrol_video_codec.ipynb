{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a90cec8-00eb-44e5-a62b-244fcb32dc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/maryam.sana/anaconda3/envs/unicontrol/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "logging improved.\n",
      "Enabled sliced_attention.\n",
      "UniControlNet: Running in eps-prediction mode\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla-xformers' with 512 in_channels\n",
      "building MemoryEfficientAttnBlock with 512 in_channels...\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 8 heads.\n",
      "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 768 and using 8 heads.\n",
      "Loaded model config from [configs/vimeo_lpips/uni_v15.yaml]\n",
      "Traceback (most recent call last):\n",
      "  File \"utils/prepare_weights.py\", line 85, in <module>\n",
      "    integrate(sys.argv[2], sys.argv[3], sys.argv[4], sys.argv[5])\n",
      "  File \"utils/prepare_weights.py\", line 63, in integrate\n",
      "    target_dict[sk] = local_weights[sk].clone()\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.68 GiB total capacity; 15.00 GiB already allocated; 16.12 MiB free; 15.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "!python utils/prepare_weights.py integrate checkpoints/vimeo_lpips_01/local-best-checkpoint-v1.ckpt ckpt/init_global.ckpt configs/vimeo_lpips/uni_v15.yaml checkpoints/vimeo_lpips_01/uni_v1.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "691f4659-3f44-4c69-b61c-b2e2f50e68de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from pytorch_lightning import seed_everything\n",
    "import glob\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from src.test.test_codec import process_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1918d838-ee92-4c33-84d8-cd818117499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot original vs predicted images and save the plot\n",
    "def plot_images(original_images, predictions, save_location, start_index=4, end_index=9, dpi=300):\n",
    "    fig, axes = plt.subplots(2, end_index - start_index, figsize=(30, 10))\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        axes[0, i - start_index].imshow(original_images[i])\n",
    "        axes[0, i - start_index].set_title(f\"Original {i + 1}\")\n",
    "        axes[0, i - start_index].axis('off')\n",
    "        axes[1, i - start_index].imshow(predictions[i])\n",
    "        axes[1, i - start_index].set_title(f\"Prediction {i + 1}\")\n",
    "        axes[1, i - start_index].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot as a high-quality image\n",
    "    plt.savefig(save_location, dpi=dpi, bbox_inches='tight')\n",
    "    print(f\"Plot saved at {save_location}\")\n",
    "\n",
    "    # Optionally close the figure to free up memory\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3b42ace-62d0-4e70-9058-42633f324089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "Selected timesteps for ddim sampler: [  1  21  41  61  81 101 121 141 161 181 201 221 241 261 281 301 321 341\n",
      " 361 381 401 421 441 461 481 501 521 541 561 581 601 621 641 661 681 701\n",
      " 721 741 761 781 801 821 841 861 881 901 921 941 961 981]\n",
      "Selected alphas for ddim sampler: a_t: tensor([0.9983, 0.9804, 0.9609, 0.9398, 0.9171, 0.8930, 0.8674, 0.8404, 0.8121,\n",
      "        0.7827, 0.7521, 0.7207, 0.6885, 0.6557, 0.6224, 0.5888, 0.5551, 0.5215,\n",
      "        0.4882, 0.4552, 0.4229, 0.3913, 0.3605, 0.3308, 0.3023, 0.2750, 0.2490,\n",
      "        0.2245, 0.2014, 0.1799, 0.1598, 0.1413, 0.1243, 0.1087, 0.0946, 0.0819,\n",
      "        0.0705, 0.0604, 0.0514, 0.0435, 0.0365, 0.0305, 0.0254, 0.0210, 0.0172,\n",
      "        0.0140, 0.0113, 0.0091, 0.0073, 0.0058]); a_(t-1): [0.99914998 0.99829602 0.98038077 0.96087277 0.93978298 0.91713792\n",
      " 0.89298052 0.86737001 0.84038192 0.81210774 0.78265446 0.75214338\n",
      " 0.72070938 0.68849909 0.65566933 0.62238538 0.58881873 0.55514455\n",
      " 0.52153981 0.4881804  0.45523876 0.42288151 0.39126703 0.36054322\n",
      " 0.33084565 0.30229566 0.27499905 0.24904492 0.22450483 0.20143245\n",
      " 0.1798636  0.15981644 0.14129217 0.12427604 0.10873855 0.09463691\n",
      " 0.08191671 0.0705137  0.06035557 0.05136392 0.043456   0.03654652\n",
      " 0.03054927 0.02537862 0.02095082 0.01718517 0.0140049  0.01133791\n",
      " 0.00911731 0.00728173]\n",
      "For the chosen value of eta, which is 0.0, this results in the following sigma_t schedule for ddim sampler tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], dtype=torch.float64)\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|█████████████████████████████████████████| 50/50 [00:12<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved prediction image: predictions/lpips_01/im2_pred.png\n"
     ]
    }
   ],
   "source": [
    "# Define folders for the images\n",
    "original_folder = \"/data/maryam.sana/vimeo_unicontrol/Uni-ControlNet/data/UVG/Beauty\"\n",
    "canny_folder = \"/data/maryam.sana/vimeo_unicontrol/Uni-ControlNet/data/UVG/optical_flow/Beauty\"\n",
    "previous_frame_folder = \"/data/maryam.sana/vimeo_unicontrol/Uni-ControlNet/data/UVG/Beauty/quality_8\"\n",
    "pred_folder = \"predictions/lpips_01/\"\n",
    "\n",
    "# Retrieve image paths\n",
    "image_paths = sorted(glob.glob(os.path.join(original_folder, \"*.png\")))  # Adjust extension if needed (e.g., .jpg)\n",
    "canny_paths = sorted(glob.glob(os.path.join(canny_folder, \"*.png\")))     # Adjust extension if needed\n",
    "previous_frames_paths = sorted(glob.glob(os.path.join(previous_frame_folder, \"*.png\")))  # Adjust extension if needed\n",
    "\n",
    "# Define the prompt for processing\n",
    "prompt = \"A beautiful blonde girl with pink lipstick\"\n",
    "# prompt = 'A jockey sitting on top of a brown horse running a race track'\n",
    "\n",
    "# Number of images you want to process\n",
    "num_images = 2\n",
    "\n",
    "# Call the function to process the images and get predictions\n",
    "original_images, predictions = process_images(\n",
    "    image_paths=image_paths,\n",
    "    canny_paths=canny_paths,\n",
    "    prompt=prompt,\n",
    "    previous_frames_paths=previous_frames_paths,\n",
    "    pred_folder=pred_folder,\n",
    "    num_images=num_images\n",
    ")\n",
    "\n",
    "# Define the save location for the plot\n",
    "# plot_save_location = os.path.join(pred_folder, \"original_vs_predicted.png\")\n",
    "\n",
    "# # Plot the images and save the plot\n",
    "# plot_images(\n",
    "#     original_images=original_images,\n",
    "#     predictions=predictions,\n",
    "#     save_location=plot_save_location,\n",
    "#     start_index=2,  # Customize the range of images for plotting\n",
    "#     end_index=6,\n",
    "#     dpi=300\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18eec73-8210-47c6-8ae2-a0e18df57848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from test_utils import calculate_metrics_batch\n",
    "\n",
    "# Define the dataset directories\n",
    "original_root = \"data/UVG\"\n",
    "pred_root = \"predictions/final\"\n",
    "\n",
    "# Video names to process\n",
    "# videos = [\"Beauty\", \"Bosphorus\", \"Jockey\"]\n",
    "videos = [ \"Bosphorus\"]\n",
    "\n",
    "# Initialize a dictionary to store metrics for each video\n",
    "all_metrics = {}\n",
    "\n",
    "# Loop through each video\n",
    "for video in videos:\n",
    "    original_images = []\n",
    "    pred_images = []\n",
    "\n",
    "    # Load images from im2 to im10\n",
    "    for i in range(2, 11):\n",
    "        original_path = os.path.join(original_root, video, f\"im{i:05d}.png\")  # Format: im00002.png, im00003.png, etc.\n",
    "        # print(original_path)\n",
    "        pred_path = os.path.join(pred_root,video, f\"im{i}_pred.png\")  # Format: Beauty_im2_pred.png, etc.\n",
    "        # print(pred_path)\n",
    "        # Load the images and append to lists\n",
    "        if os.path.exists(original_path) and os.path.exists(pred_path):\n",
    "            original_images.append(Image.open(original_path).convert(\"RGB\"))\n",
    "            pred_images.append(Image.open(pred_path).convert(\"RGB\"))\n",
    "        else:\n",
    "            print(f\"Warning: Missing image for {video} frame {i}\")\n",
    "\n",
    "    # Calculate metrics for the batch of images\n",
    "    if original_images and pred_images:\n",
    "        metrics = calculate_metrics_batch(original_images, pred_images)\n",
    "        all_metrics[video] = metrics\n",
    "        print(f\"Metrics for {video}:\", metrics)\n",
    "    else:\n",
    "        print(f\"No images found for video {video}\")\n",
    "\n",
    "# Print all metrics for each video\n",
    "print(\"\\nFinal Metrics Summary for All Videos:\")\n",
    "for video, metrics in all_metrics.items():\n",
    "    print(f\"{video} Metrics: {metrics}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7715ea92-7520-47a4-ae9a-87b258fae9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_images = [Image.open(\"data/UVG/Beauty/im00001.png\").convert(\"RGB\"), \n",
    "                   Image.open(\"data/UVG/Beauty/im00002.png\").convert(\"RGB\"),\n",
    "                   Image.open(\"data/UVG/Beauty/im00003.png\").convert(\"RGB\"),\n",
    "                   Image.open(\"data/UVG/Beauty/im00004.png\").convert(\"RGB\"),\n",
    "                   Image.open(\"data/UVG/Beauty/im00005.png\").convert(\"RGB\")]\n",
    "pred_images = [Image.open(\"data/UVG/Beauty/quality_8/im00001.png\").convert(\"RGB\"), \n",
    "                   Image.open(\"data/UVG/Beauty/quality_8/im00002.png\").convert(\"RGB\"),\n",
    "                   Image.open(\"data/UVG/Beauty/quality_8/im00003.png\").convert(\"RGB\"),\n",
    "                   Image.open(\"data/UVG/Beauty/quality_8/im00004.png\").convert(\"RGB\"),\n",
    "                   Image.open(\"data/UVG/Beauty/quality_8/im00005.png\").convert(\"RGB\")]\n",
    "\n",
    "metrics = calculate_metrics_batch(original_images, pred_images)\n",
    "print(\"Batch Metrics:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2a05a0-a3ef-4f17-bd98-e6f3f1d655c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Define paths for the original and predicted images\n",
    "original_path = 'data/UVG/Beauty/im00001.png'\n",
    "prediction_8_path = 'predictions_8/im1_pred.png'\n",
    "prediction_8_v8_path = 'pred_8_v8/Beauty/im1_pred.png'\n",
    "\n",
    "# Metrics for each prediction\n",
    "metrics_8 = {\n",
    "    'PSNR': 15.96, \n",
    "    'MS-SSIM': 0.8374, \n",
    "    'LPIPS': 0.2572, \n",
    "    'FID': 5.9782\n",
    "}\n",
    "\n",
    "metrics_8_v8 = {\n",
    "    'PSNR': 26.118643, \n",
    "    'MS-SSIM': 0.91220, \n",
    "    'LPIPS': 0.09480324, \n",
    "    'FID': 0.264220\n",
    "}\n",
    "\n",
    "# Load images\n",
    "original_image = Image.open(original_path)\n",
    "prediction_8_image = Image.open(prediction_8_path)\n",
    "prediction_8_v8_image = Image.open(prediction_8_v8_path)\n",
    "\n",
    "# Create a figure with 1 row and 3 columns\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle('Original and Predicted Images with Metrics')\n",
    "\n",
    "# Display original image\n",
    "axes[0].imshow(original_image)\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Display prediction from predictions_8\n",
    "axes[1].imshow(prediction_8_image)\n",
    "axes[1].set_title('Prediction from v1 Training')\n",
    "axes[1].text(\n",
    "    0.5, -0.15, \n",
    "    f\"PSNR: {metrics_8['PSNR']}\\nMS-SSIM: {metrics_8['MS-SSIM']}\\nLPIPS: {metrics_8['LPIPS']}\\nFID: {metrics_8['FID']}\", \n",
    "    ha='center', va='top', transform=axes[1].transAxes, fontsize=12\n",
    ")\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Display prediction from pred_8_v8\n",
    "axes[2].imshow(prediction_8_v8_image)\n",
    "axes[2].set_title('Prediction from v2 Training')\n",
    "axes[2].text(\n",
    "    0.5, -0.15, \n",
    "    f\"PSNR: {metrics_8_v8['PSNR']}\\nMS-SSIM: {metrics_8_v8['MS-SSIM']}\\nLPIPS: {metrics_8_v8['LPIPS']}\\nFID: {metrics_8_v8['FID']}\", \n",
    "    ha='center', va='top', transform=axes[2].transAxes, fontsize=12\n",
    ")\n",
    "axes[2].axis('off')\n",
    "\n",
    "# Adjust layout and show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccd492c-3409-44b1-bd90-d366a8b75dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def display_high_res_plot(original_path, pred_q1_path, pred_q4_path, pred_q8_path, \n",
    "                          psnr_q1, mssim_q1, psnr_q4, mssim_q4, psnr_q8, mssim_q8, \n",
    "                          save_path='output_plot.png', figsize=(20, 15)):\n",
    "    \"\"\"\n",
    "    Display a high resolution plot with original image and predictions for quality 1, 4, and 8.\n",
    "    Also display PSNR and MS-SSIM values under each predicted image in bold.\n",
    "    Save the plot with dpi=300.\n",
    "    \"\"\"\n",
    "    # Load images\n",
    "    original = Image.open(original_path)\n",
    "    pred_q1 = Image.open(pred_q1_path)\n",
    "    pred_q4 = Image.open(pred_q4_path)\n",
    "    pred_q8 = Image.open(pred_q8_path)\n",
    "\n",
    "    # Create a figure with high resolution and black background\n",
    "    fig, axes = plt.subplots(1, 4, figsize=figsize)\n",
    "    fig.patch.set_facecolor('black')  # Set background color to black\n",
    "\n",
    "    # Set black background for individual axes\n",
    "    for ax in axes:\n",
    "        ax.set_facecolor('black')\n",
    "\n",
    "    # Display original image\n",
    "    axes[0].imshow(original)\n",
    "    axes[0].set_title(\"Original Image\", color='white')  # Title in white color\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # Display Quality 1 prediction\n",
    "    axes[1].imshow(pred_q1)\n",
    "    axes[1].set_title(\"Quality 1 Prediction\", color='white')\n",
    "    axes[1].axis('off')\n",
    "    # Add PSNR and MS-SSIM as text under the image in bold and on two separate lines\n",
    "    axes[1].text(0.5, -0.15, f\"PSNR: {psnr_q1:.2f}\\nMS-SSIM: {mssim_q1:.4f}\", \n",
    "                 size=12, ha=\"center\", color='white', transform=axes[1].transAxes, weight='bold')\n",
    "\n",
    "    # Display Quality 4 prediction\n",
    "    axes[2].imshow(pred_q4)\n",
    "    axes[2].set_title(\"Quality 4 Prediction\", color='white')\n",
    "    axes[2].axis('off')\n",
    "    # Add PSNR and MS-SSIM as text under the image in bold and on two separate lines\n",
    "    axes[2].text(0.5, -0.15, f\"PSNR: {psnr_q4:.2f}\\nMS-SSIM: {mssim_q4:.4f}\", \n",
    "                 size=12, ha=\"center\", color='white', transform=axes[2].transAxes, weight='bold')\n",
    "\n",
    "    # Display Quality 8 prediction\n",
    "    axes[3].imshow(pred_q8)\n",
    "    axes[3].set_title(\"Quality 8 Prediction\", color='white')\n",
    "    axes[3].axis('off')\n",
    "    # Add PSNR and MS-SSIM as text under the image in bold and on two separate lines\n",
    "    axes[3].text(0.5, -0.15, f\"PSNR: {psnr_q8:.2f}\\nMS-SSIM: {mssim_q8:.4f}\", \n",
    "                 size=12, ha=\"center\", color='white', transform=axes[3].transAxes, weight='bold')\n",
    "\n",
    "    # Adjust layout for better display\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot with high DPI\n",
    "    plt.savefig(save_path, dpi=300, facecolor=fig.get_facecolor())\n",
    "    print(f\"Figure saved as {save_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "original_path = 'data/UVG/Beauty/im00004.png'\n",
    "pred_q1_path = 'predictions_1/im4_pred.png'\n",
    "pred_q4_path = 'predictions_4/im4_pred.png'\n",
    "pred_q8_path = 'predictions_8/im4_pred.png'\n",
    "\n",
    "# PSNR and MS-SSIM values for predictions (provided values)\n",
    "psnr_q1, mssim_q1 = 14.77, 0.7226\n",
    "psnr_q4, mssim_q4 = 12.92, 0.6222\n",
    "psnr_q8, mssim_q8 = 15.80, 0.8594\n",
    "\n",
    "# Call the function to display the images with metrics and save the plot\n",
    "display_high_res_plot(original_path, pred_q1_path, pred_q4_path, pred_q8_path, \n",
    "                      psnr_q1, mssim_q1, psnr_q4, mssim_q4, psnr_q8, mssim_q8,\n",
    "                      save_path='output_comparison.png')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
